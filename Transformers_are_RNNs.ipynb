{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers are RNNs",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib8YRy5Zm1Pv"
      },
      "source": [
        "# Transformers are RNNs Demo\n",
        "\n",
        "This is a colab to accompany our [project page](https://linear-transformers.com/)\n",
        "and to explore the transformer formulation developed in our paper [Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/abs/2006.16236).\n",
        "\n",
        "The colab is structured in three sections.\n",
        "1. Firstly, we install the `fast_transformers` library and create an example transformer.\n",
        "2. Secondly, we load the models we trained for autoregressive image prediction and reproduce parts of the experiment in section 4.2 in the paper.\n",
        "3. Finally, we perform some benchmarking comparing the computational requirements for linear attention and softmax attention.\n",
        "\n",
        "If you want to run the CUDA part of the demo, make sure that you enable CUDA acceleration via **Runtime -> Change runtime** or **Edit -> Notebook settings**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbhxqvqHxcDa"
      },
      "source": [
        "# Installation and First Steps\n",
        "\n",
        "The installation is directly from PyPI. This will take several minutes since it compiles several custom CUDA kernels, not only for linear autoregressive attention. Maybe grab a coffee (if you are into these things)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngOSLpE2xtz8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b4512d-406e-4a6f-9629-e2a864a2fafc"
      },
      "source": [
        "!pip install -v pytorch-fast-transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-x3cs0xno\n",
            "Created temporary directory: /tmp/pip-req-tracker-g3xf8g83\n",
            "Created requirements tracker '/tmp/pip-req-tracker-g3xf8g83'\n",
            "Created temporary directory: /tmp/pip-install-uryulpbk\n",
            "1 location(s) to search for versions of pytorch-fast-transformers:\n",
            "* https://pypi.org/simple/pytorch-fast-transformers/\n",
            "Getting page https://pypi.org/simple/pytorch-fast-transformers/\n",
            "Found index url https://pypi.org/simple\n",
            "Looking up \"https://pypi.org/simple/pytorch-fast-transformers/\" in the cache\n",
            "Request header has \"max_age\" as 0, cache bypassed\n",
            "Starting new HTTPS connection (1): pypi.org:443\n",
            "https://pypi.org:443 \"GET /simple/pytorch-fast-transformers/ HTTP/1.1\" 200 980\n",
            "Updating cache with response from \"https://pypi.org/simple/pytorch-fast-transformers/\"\n",
            "Caching due to etag\n",
            "Analyzing links from page https://pypi.org/simple/pytorch-fast-transformers/\n",
            "  Found link https://files.pythonhosted.org/packages/25/95/f9b2bdee5bc35e2c3b44cb2847f7bc77ddecd89aae74c4f86f8a9451c78f/pytorch-fast-transformers-0.1.1.tar.gz#sha256=c2823a900eb5275f7ec2f35681dccd7a6adf853206dbcb70973ada4cee098296 (from https://pypi.org/simple/pytorch-fast-transformers/), version: 0.1.1\n",
            "  Found link https://files.pythonhosted.org/packages/00/40/26426232fa1697bfd681cf5ab3c784f13d72028b6e132d84e2453fab948b/pytorch-fast-transformers-0.1.2.tar.gz#sha256=15b9a48d204e54e1eb413126c19556c766df9a1bf64dd0c3a1f09c30ffdd564e (from https://pypi.org/simple/pytorch-fast-transformers/), version: 0.1.2\n",
            "  Found link https://files.pythonhosted.org/packages/4d/e9/7c352eb727b87ea71b23f00a52de80d40dc0398937a82044692eb26a2fb7/pytorch-fast-transformers-0.1.3.tar.gz#sha256=cf89b628bb4defc016a8fbc109b94d87a877f449a9d1ae2bb6b3a0fd1fb08c4c (from https://pypi.org/simple/pytorch-fast-transformers/), version: 0.1.3\n",
            "  Found link https://files.pythonhosted.org/packages/12/7d/9899e7add42cd7852b765580bee1a9c31d40775833dccfc800d2cc33aa7e/pytorch-fast-transformers-0.2.0.tar.gz#sha256=202f9b10f42ab006848c7f58d2f35cbc6e33ec90fff3b82b3750f52fb6a05bb0 (from https://pypi.org/simple/pytorch-fast-transformers/), version: 0.2.0\n",
            "  Found link https://files.pythonhosted.org/packages/d3/12/998209a95d523bdd42beff04e04b7bdad9b336e91a192fabe67c233a165f/pytorch-fast-transformers-0.2.1.tar.gz#sha256=976fd2198a6bc1ddbc5f730212f9d41b6ebfa7f50b87ab10a492603369db2944 (from https://pypi.org/simple/pytorch-fast-transformers/), version: 0.2.1\n",
            "  Found link https://files.pythonhosted.org/packages/01/b6/4f046de9852b88c98ededc7905d01cb25a945337068a03e3213009c843bb/pytorch-fast-transformers-0.2.2.tar.gz#sha256=1a97f52ac293de7cd96ddcc0daa6bf5b0c98f1811fbb43429571bee7deef7f97 (from https://pypi.org/simple/pytorch-fast-transformers/), version: 0.2.2\n",
            "  Found link https://files.pythonhosted.org/packages/03/9b/38905999695b381a1e239b91afce219892a23614248fc024e04558f36237/pytorch-fast-transformers-0.3.0.tar.gz#sha256=6811900db71babd232b0ccf01cd199cc788d288789efb592280f1190da2dd41a (from https://pypi.org/simple/pytorch-fast-transformers/), version: 0.3.0\n",
            "  Found link https://files.pythonhosted.org/packages/ea/bc/00f597fefeab6341114c41045c1b232c38436738e0e8eac1bc9d5e9d5962/pytorch-fast-transformers-0.4.0.tar.gz#sha256=d1826bc31b9dfbcd018998b897667e89fc6566bd3f8c424cda9f0943544f7e90 (from https://pypi.org/simple/pytorch-fast-transformers/), version: 0.4.0\n",
            "Given no hashes to check 8 links for project 'pytorch-fast-transformers': discarding no candidates\n",
            "Using version 0.4.0 (newest of versions: 0.1.1, 0.1.2, 0.1.3, 0.2.0, 0.2.1, 0.2.2, 0.3.0, 0.4.0)\n",
            "Collecting pytorch-fast-transformers\n",
            "  Created temporary directory: /tmp/pip-unpack-4e6q3beg\n",
            "  Looking up \"https://files.pythonhosted.org/packages/ea/bc/00f597fefeab6341114c41045c1b232c38436738e0e8eac1bc9d5e9d5962/pytorch-fast-transformers-0.4.0.tar.gz\" in the cache\n",
            "  No cache entry available\n",
            "  Starting new HTTPS connection (1): files.pythonhosted.org:443\n",
            "  https://files.pythonhosted.org:443 \"GET /packages/ea/bc/00f597fefeab6341114c41045c1b232c38436738e0e8eac1bc9d5e9d5962/pytorch-fast-transformers-0.4.0.tar.gz HTTP/1.1\" 200 93616\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/bc/00f597fefeab6341114c41045c1b232c38436738e0e8eac1bc9d5e9d5962/pytorch-fast-transformers-0.4.0.tar.gz (93kB)\n",
            "\u001b[K     |███████████████████████████████▌| 92kB 9.5MB/s eta 0:00:01  Ignoring unknown cache-control directive: immutable\n",
            "  Updating cache with response from \"https://files.pythonhosted.org/packages/ea/bc/00f597fefeab6341114c41045c1b232c38436738e0e8eac1bc9d5e9d5962/pytorch-fast-transformers-0.4.0.tar.gz\"\n",
            "  Caching due to etag\n",
            "\u001b[K     |████████████████████████████████| 102kB 6.0MB/s \n",
            "\u001b[?25h  Added pytorch-fast-transformers from https://files.pythonhosted.org/packages/ea/bc/00f597fefeab6341114c41045c1b232c38436738e0e8eac1bc9d5e9d5962/pytorch-fast-transformers-0.4.0.tar.gz#sha256=d1826bc31b9dfbcd018998b897667e89fc6566bd3f8c424cda9f0943544f7e90 to build tracker '/tmp/pip-req-tracker-g3xf8g83'\n",
            "    Running setup.py (path:/tmp/pip-install-uryulpbk/pytorch-fast-transformers/setup.py) egg_info for package pytorch-fast-transformers\n",
            "    Running command python setup.py egg_info\n",
            "    running egg_info\n",
            "    creating /tmp/pip-install-uryulpbk/pytorch-fast-transformers/pip-egg-info/pytorch_fast_transformers.egg-info\n",
            "    writing /tmp/pip-install-uryulpbk/pytorch-fast-transformers/pip-egg-info/pytorch_fast_transformers.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-install-uryulpbk/pytorch-fast-transformers/pip-egg-info/pytorch_fast_transformers.egg-info/dependency_links.txt\n",
            "    writing requirements to /tmp/pip-install-uryulpbk/pytorch-fast-transformers/pip-egg-info/pytorch_fast_transformers.egg-info/requires.txt\n",
            "    writing top-level names to /tmp/pip-install-uryulpbk/pytorch-fast-transformers/pip-egg-info/pytorch_fast_transformers.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-install-uryulpbk/pytorch-fast-transformers/pip-egg-info/pytorch_fast_transformers.egg-info/SOURCES.txt'\n",
            "    reading manifest file '/tmp/pip-install-uryulpbk/pytorch-fast-transformers/pip-egg-info/pytorch_fast_transformers.egg-info/SOURCES.txt'\n",
            "    writing manifest file '/tmp/pip-install-uryulpbk/pytorch-fast-transformers/pip-egg-info/pytorch_fast_transformers.egg-info/SOURCES.txt'\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/utils/cpp_extension.py:369: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "  Source in /tmp/pip-install-uryulpbk/pytorch-fast-transformers has version 0.4.0, which satisfies requirement pytorch-fast-transformers from https://files.pythonhosted.org/packages/ea/bc/00f597fefeab6341114c41045c1b232c38436738e0e8eac1bc9d5e9d5962/pytorch-fast-transformers-0.4.0.tar.gz#sha256=d1826bc31b9dfbcd018998b897667e89fc6566bd3f8c424cda9f0943544f7e90\n",
            "  Removed pytorch-fast-transformers from https://files.pythonhosted.org/packages/ea/bc/00f597fefeab6341114c41045c1b232c38436738e0e8eac1bc9d5e9d5962/pytorch-fast-transformers-0.4.0.tar.gz#sha256=d1826bc31b9dfbcd018998b897667e89fc6566bd3f8c424cda9f0943544f7e90 from build tracker '/tmp/pip-req-tracker-g3xf8g83'\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pytorch-fast-transformers) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-fast-transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-fast-transformers) (3.7.4.3)\n",
            "Building wheels for collected packages: pytorch-fast-transformers\n",
            "  Created temporary directory: /tmp/pip-wheel-vy9ymwfh\n",
            "  Building wheel for pytorch-fast-transformers (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-vy9ymwfh\n",
            "  Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-uryulpbk/pytorch-fast-transformers/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-uryulpbk/pytorch-fast-transformers/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-vy9ymwfh --python-tag cp37\n",
            "  running bdist_wheel\n",
            "  /usr/local/lib/python3.7/dist-packages/torch/utils/cpp_extension.py:369: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "    warnings.warn(msg.format('we could not find ninja.'))\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib.linux-x86_64-3.7\n",
            "  creating build/lib.linux-x86_64-3.7/fast_transformers\n",
            "  copying fast_transformers/masking.py -> build/lib.linux-x86_64-3.7/fast_transformers\n",
            "  copying fast_transformers/transformers.py -> build/lib.linux-x86_64-3.7/fast_transformers\n",
            "  copying fast_transformers/weight_mapper.py -> build/lib.linux-x86_64-3.7/fast_transformers\n",
            "  copying fast_transformers/__init__.py -> build/lib.linux-x86_64-3.7/fast_transformers\n",
            "  copying fast_transformers/utils.py -> build/lib.linux-x86_64-3.7/fast_transformers\n",
            "  creating build/lib.linux-x86_64-3.7/fast_transformers/aggregate\n",
            "  copying fast_transformers/aggregate/__init__.py -> build/lib.linux-x86_64-3.7/fast_transformers/aggregate\n",
            "  creating build/lib.linux-x86_64-3.7/fast_transformers/attention_registry\n",
            "  copying fast_transformers/attention_registry/registry.py -> build/lib.linux-x86_64-3.7/fast_transformers/attention_registry\n",
            "  copying fast_transformers/attention_registry/spec.py -> build/lib.linux-x86_64-3.7/fast_transformers/attention_registry\n",
            "  copying fast_transformers/attention_registry/__init__.py -> build/lib.linux-x86_64-3.7/fast_transformers/attention_registry\n",
            "  creating build/lib.linux-x86_64-3.7/fast_transformers/clustering\n",
            "  copying fast_transformers/clustering/__init__.py -> build/lib.linux-x86_64-3.7/fast_transformers/clustering\n",
            "  creating build/lib.linux-x86_64-3.7/fast_transformers/causal_product\n",
            "  copying fast_transformers/causal_product/__init__.py -> build/lib.linux-x86_64-3.7/fast_transformers/causal_product\n",
            "  creating build/lib.linux-x86_64-3.7/fast_transformers/attention\n",
            "  copying fast_transformers/attention/full_attention.py -> build/lib.linux-x86_64-3.7/fast_transformers/attention\n",
            "  copying fast_transformers/attention/exact_topk_attention.py -> build/lib.linux-x86_64-3.7/fast_transformers/attention\n",
            "  copying fast_transformers/attention/clustered_attention.py -> build/lib.linux-x86_64-3.7/fast_transformers/attention\n",
            "  copying fast_transformers/attention/improved_clustered_causal_attention.py -> build/lib.linux-x86_64-3.7/fast_transformers/attention\n",
            "  copying fast_transformers/attention/causal_linear_attention.py -> build/lib.linux-x86_64-3.7/fast_transformers/attention\n",
            "  copying fast_transformers/attention/improved_clustered_attention.py -> build/lib.linux-x86_64-3.7/fast_transformers/attention\n",
            "  copying fast_transformers/attention/local_attention.py -> build/lib.linux-x86_64-3.7/fast_transformers/attention\n",
            "  copying fast_transformers/attention/conditional_full_attention.py -> build/lib.linux-x86_64-3.7/fast_transformers/attention\n",
            "  copying fast_transformers/attention/reformer_attention.py -> build/lib.linux-x86_64-3.7/fast_transformers/attention\n",
            "  copying fast_transformers/attention/attention_layer.py -> build/lib.linux-x86_64-3.7/fast_transformers/attention\n",
            "  copying fast_transformers/attention/__init__.py -> build/lib.linux-x86_64-3.7/fast_transformers/attention\n",
            "  copying fast_transformers/attention/linear_attention.py -> build/lib.linux-x86_64-3.7/fast_transformers/attention\n",
            "  creating build/lib.linux-x86_64-3.7/fast_transformers/recurrent\n",
            "  copying fast_transformers/recurrent/transformers.py -> build/lib.linux-x86_64-3.7/fast_transformers/recurrent\n",
            "  copying fast_transformers/recurrent/_utils.py -> build/lib.linux-x86_64-3.7/fast_transformers/recurrent\n",
            "  copying fast_transformers/recurrent/__init__.py -> build/lib.linux-x86_64-3.7/fast_transformers/recurrent\n",
            "  creating build/lib.linux-x86_64-3.7/fast_transformers/feature_maps\n",
            "  copying fast_transformers/feature_maps/base.py -> build/lib.linux-x86_64-3.7/fast_transformers/feature_maps\n",
            "  copying fast_transformers/feature_maps/fourier_features.py -> build/lib.linux-x86_64-3.7/fast_transformers/feature_maps\n",
            "  copying fast_transformers/feature_maps/__init__.py -> build/lib.linux-x86_64-3.7/fast_transformers/feature_maps\n",
            "  creating build/lib.linux-x86_64-3.7/fast_transformers/events\n",
            "  copying fast_transformers/events/event.py -> build/lib.linux-x86_64-3.7/fast_transformers/events\n",
            "  copying fast_transformers/events/filters.py -> build/lib.linux-x86_64-3.7/fast_transformers/events\n",
            "  copying fast_transformers/events/__init__.py -> build/lib.linux-x86_64-3.7/fast_transformers/events\n",
            "  copying fast_transformers/events/event_dispatcher.py -> build/lib.linux-x86_64-3.7/fast_transformers/events\n",
            "  creating build/lib.linux-x86_64-3.7/fast_transformers/builders\n",
            "  copying fast_transformers/builders/attention_builders.py -> build/lib.linux-x86_64-3.7/fast_transformers/builders\n",
            "  copying fast_transformers/builders/base.py -> build/lib.linux-x86_64-3.7/fast_transformers/builders\n",
            "  copying fast_transformers/builders/transformer_builders.py -> build/lib.linux-x86_64-3.7/fast_transformers/builders\n",
            "  copying fast_transformers/builders/__init__.py -> build/lib.linux-x86_64-3.7/fast_transformers/builders\n",
            "  creating build/lib.linux-x86_64-3.7/fast_transformers/local_product\n",
            "  copying fast_transformers/local_product/__init__.py -> build/lib.linux-x86_64-3.7/fast_transformers/local_product\n",
            "  creating build/lib.linux-x86_64-3.7/fast_transformers/sparse_product\n",
            "  copying fast_transformers/sparse_product/__init__.py -> build/lib.linux-x86_64-3.7/fast_transformers/sparse_product\n",
            "  creating build/lib.linux-x86_64-3.7/fast_transformers/hashing\n",
            "  copying fast_transformers/hashing/__init__.py -> build/lib.linux-x86_64-3.7/fast_transformers/hashing\n",
            "  creating build/lib.linux-x86_64-3.7/fast_transformers/clustering/hamming\n",
            "  copying fast_transformers/clustering/hamming/__init__.py -> build/lib.linux-x86_64-3.7/fast_transformers/clustering/hamming\n",
            "  creating build/lib.linux-x86_64-3.7/fast_transformers/recurrent/attention\n",
            "  copying fast_transformers/recurrent/attention/__init__.py -> build/lib.linux-x86_64-3.7/fast_transformers/recurrent/attention\n",
            "  creating build/lib.linux-x86_64-3.7/fast_transformers/recurrent/attention/cross_attention\n",
            "  copying fast_transformers/recurrent/attention/cross_attention/full_attention.py -> build/lib.linux-x86_64-3.7/fast_transformers/recurrent/attention/cross_attention\n",
            "  copying fast_transformers/recurrent/attention/cross_attention/attention_layer.py -> build/lib.linux-x86_64-3.7/fast_transformers/recurrent/attention/cross_attention\n",
            "  copying fast_transformers/recurrent/attention/cross_attention/__init__.py -> build/lib.linux-x86_64-3.7/fast_transformers/recurrent/attention/cross_attention\n",
            "  copying fast_transformers/recurrent/attention/cross_attention/linear_attention.py -> build/lib.linux-x86_64-3.7/fast_transformers/recurrent/attention/cross_attention\n",
            "  creating build/lib.linux-x86_64-3.7/fast_transformers/recurrent/attention/self_attention\n",
            "  copying fast_transformers/recurrent/attention/self_attention/full_attention.py -> build/lib.linux-x86_64-3.7/fast_transformers/recurrent/attention/self_attention\n",
            "  copying fast_transformers/recurrent/attention/self_attention/attention_layer.py -> build/lib.linux-x86_64-3.7/fast_transformers/recurrent/attention/self_attention\n",
            "  copying fast_transformers/recurrent/attention/self_attention/__init__.py -> build/lib.linux-x86_64-3.7/fast_transformers/recurrent/attention/self_attention\n",
            "  copying fast_transformers/recurrent/attention/self_attention/linear_attention.py -> build/lib.linux-x86_64-3.7/fast_transformers/recurrent/attention/self_attention\n",
            "  running build_ext\n",
            "  building 'fast_transformers.hashing.hash_cpu' extension\n",
            "  creating build/temp.linux-x86_64-3.7\n",
            "  creating build/temp.linux-x86_64-3.7/fast_transformers\n",
            "  creating build/temp.linux-x86_64-3.7/fast_transformers/hashing\n",
            "  x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/include/python3.7m -c fast_transformers/hashing/hash_cpu.cpp -o build/temp.linux-x86_64-3.7/fast_transformers/hashing/hash_cpu.o -fopenmp -ffast-math -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=hash_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/fast_transformers/hashing/hash_cpu.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.7/fast_transformers/hashing/hash_cpu.cpython-37m-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.aggregate.aggregate_cpu' extension\n",
            "  creating build/temp.linux-x86_64-3.7/fast_transformers/aggregate\n",
            "  x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/include/python3.7m -c fast_transformers/aggregate/aggregate_cpu.cpp -o build/temp.linux-x86_64-3.7/fast_transformers/aggregate/aggregate_cpu.o -fopenmp -ffast-math -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=aggregate_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/fast_transformers/aggregate/aggregate_cpu.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.7/fast_transformers/aggregate/aggregate_cpu.cpython-37m-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.clustering.hamming.cluster_cpu' extension\n",
            "  creating build/temp.linux-x86_64-3.7/fast_transformers/clustering\n",
            "  creating build/temp.linux-x86_64-3.7/fast_transformers/clustering/hamming\n",
            "  x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/include/python3.7m -c fast_transformers/clustering/hamming/cluster_cpu.cpp -o build/temp.linux-x86_64-3.7/fast_transformers/clustering/hamming/cluster_cpu.o -fopenmp -ffast-math -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=cluster_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/fast_transformers/clustering/hamming/cluster_cpu.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.7/fast_transformers/clustering/hamming/cluster_cpu.cpython-37m-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.sparse_product.sparse_product_cpu' extension\n",
            "  creating build/temp.linux-x86_64-3.7/fast_transformers/sparse_product\n",
            "  x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/include/python3.7m -c fast_transformers/sparse_product/sparse_product_cpu.cpp -o build/temp.linux-x86_64-3.7/fast_transformers/sparse_product/sparse_product_cpu.o -fopenmp -ffast-math -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=sparse_product_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWRa796oArUL"
      },
      "source": [
        "Let's validate our freshly installed package by creating a small transformer encoder and running it on dummy data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7Xgy7ozA2x0"
      },
      "source": [
        "from fast_transformers.builders import TransformerEncoderBuilder\n",
        "from fast_transformers.masking import LengthMask, TriangularCausalMask\n",
        "import torch\n",
        "\n",
        "model = TransformerEncoderBuilder.from_kwargs(\n",
        "    n_layers=4,\n",
        "    n_heads=4,\n",
        "    feed_forward_dimensions=128,\n",
        "    query_dimensions=32,\n",
        "    value_dimensions=32,\n",
        "    attention_type=\"full\" # this means normal softmax attention\n",
        ").get()\n",
        "\n",
        "x = torch.rand(\n",
        "    10,  # batch size \n",
        "    100, # sequence length\n",
        "    128  # feature dimensions\n",
        ")\n",
        "y = model(x) # calling without masks which means attend to everything\n",
        "y = model(\n",
        "    x,\n",
        "    attn_mask=TriangularCausalMask(100),   # causal masking\n",
        "    length_mask=LengthMask(torch.tensor([\n",
        "        100, 70, 60, 30, 80, 100,          # The sequence length for every\n",
        "        50, 40, 10, 20                     # sample in the batch\n",
        "    ]))\n",
        ")\n",
        "print(\"If you reached here, everything works\", y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbod0KVZDCdi"
      },
      "source": [
        "# Autoregressive Image Generation\n",
        "\n",
        "Let us first define two pytorch modules for autoregressive image generation. One uses a recurrent formulation that accepts one input at a time and the other similar to the default PyTorch implementation accepts the whole sequence.\n",
        "\n",
        "In both cases the model is simply wrapping a transformer with an input embedding layer and a prediction layer. This becomes apparent upon reading the simple `forward()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYO-fyzuDqGu"
      },
      "source": [
        "import math\n",
        "from fast_transformers.builders import RecurrentEncoderBuilder\n",
        "\n",
        "class RecurrentGenerator(torch.nn.Module):\n",
        "    class PositionalEncoding(torch.nn.Module):\n",
        "        def __init__(self, d_model, dropout=0.0, max_len=5000):\n",
        "            super(RecurrentGenerator.PositionalEncoding, self).__init__()\n",
        "            self.dropout = torch.nn.Dropout(p=dropout)\n",
        "            self.d_model = d_model\n",
        "            pe = torch.zeros(max_len, d_model)\n",
        "            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "            pe[:, 0::2] = torch.sin(position * div_term)\n",
        "            pe[:, 1::2] = torch.cos(position * div_term)\n",
        "            pe = pe.unsqueeze(0)\n",
        "            self.register_buffer('pe', pe)\n",
        "\n",
        "        def forward(self, x, i):\n",
        "            pos_embedding =  self.pe[0, i:i+1]\n",
        "            x = torch.cat(\n",
        "                [x, pos_embedding.expand_as(x)],\n",
        "                dim=1\n",
        "            )\n",
        "            return self.dropout(x)\n",
        "\n",
        "    def __init__(self, d_model, sequence_length, mixtures,\n",
        "                 attention_type=\"full\", n_layers=4, n_heads=4,\n",
        "                 d_query=32, dropout=0.1, softmax_temp=None,\n",
        "                 attention_dropout=0.1):\n",
        "        super(RecurrentGenerator, self).__init__()\n",
        "\n",
        "        self.pos_embedding = self.PositionalEncoding(\n",
        "            d_model//2,\n",
        "            max_len=sequence_length\n",
        "        )\n",
        "        self.value_embedding = torch.nn.Embedding(\n",
        "            256,\n",
        "            d_model//2\n",
        "        )\n",
        "        self.transformer = RecurrentEncoderBuilder.from_kwargs(\n",
        "            attention_type=attention_type,\n",
        "            n_layers=n_layers,\n",
        "            n_heads=n_heads,\n",
        "            feed_forward_dimensions=n_heads*d_query*4,\n",
        "            query_dimensions=d_query,\n",
        "            value_dimensions=d_query,\n",
        "            dropout=dropout,\n",
        "            softmax_temp=softmax_temp,\n",
        "            attention_dropout=attention_dropout\n",
        "        ).get()\n",
        "        self.predictor = torch.nn.Linear(\n",
        "            d_model,\n",
        "            mixtures * 3\n",
        "        )\n",
        "\n",
        "    def forward(self, x, i=0, memory=None):\n",
        "        x = x.view(x.shape[0])\n",
        "        x = self.value_embedding(x)\n",
        "        x = self.pos_embedding(x, i)\n",
        "        y_hat, memory = self.transformer(x, memory)\n",
        "        y_hat = self.predictor(y_hat)\n",
        "\n",
        "        return y_hat, memory\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvDoMcfrFfQK"
      },
      "source": [
        "and the non recurrent one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXGOjayeFh13"
      },
      "source": [
        "class Generator(torch.nn.Module):\n",
        "    class PositionalEncoding(torch.nn.Module):\n",
        "        def __init__(self, d_model, dropout=0.0, max_len=5000):\n",
        "            super(Generator.PositionalEncoding, self).__init__()\n",
        "            self.dropout = torch.nn.Dropout(p=dropout)\n",
        "            self.d_model = d_model\n",
        "            pe = torch.zeros(max_len, d_model)\n",
        "            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "            pe[:, 0::2] = torch.sin(position * div_term)\n",
        "            pe[:, 1::2] = torch.cos(position * div_term)\n",
        "            pe = pe.unsqueeze(0)\n",
        "            self.register_buffer('pe', pe)\n",
        "\n",
        "        def forward(self, x):\n",
        "            pos_embedding =  self.pe[:, :x.size(1), :]\n",
        "            pos_embedding = torch.repeat_interleave(pos_embedding, x.shape[0], dim=0)\n",
        "            x =  torch.cat([x, pos_embedding], dim=2)\n",
        "            return self.dropout(x)\n",
        "\n",
        "    def __init__(self, d_model, sequence_length, mixtures,\n",
        "                 attention_type=\"full\", n_layers=4, n_heads=4,\n",
        "                 d_query=32, dropout=0.1, softmax_temp=None,\n",
        "                 attention_dropout=0.1):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.pos_embedding = self.PositionalEncoding(\n",
        "            d_model//2,\n",
        "            max_len=sequence_length\n",
        "        )\n",
        "        self.value_embedding = torch.nn.Embedding(\n",
        "            256,\n",
        "            d_model//2\n",
        "        )\n",
        "\n",
        "        self.transformer = TransformerEncoderBuilder.from_kwargs(\n",
        "            attention_type=attention_type,\n",
        "            n_layers=n_layers,\n",
        "            n_heads=n_heads,\n",
        "            feed_forward_dimensions=n_heads*d_query*4,\n",
        "            query_dimensions=d_query,\n",
        "            value_dimensions=d_query,\n",
        "            dropout=dropout,\n",
        "            softmax_temp=softmax_temp,\n",
        "            attention_dropout=attention_dropout\n",
        "        ).get()\n",
        "\n",
        "        hidden_size = n_heads*d_query\n",
        "        self.predictor = torch.nn.Linear(\n",
        "            hidden_size,\n",
        "            mixtures * 3\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.value_embedding(x)\n",
        "        x = self.pos_embedding(x)\n",
        "        triangular_mask = TriangularCausalMask(x.shape[1], device=x.device)\n",
        "        y_hat = self.transformer(x, attn_mask=triangular_mask)\n",
        "        y_hat = self.predictor(y_hat)\n",
        "\n",
        "        return y_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyVtDWC0PL3w"
      },
      "source": [
        "## Helper functions\n",
        "\n",
        "We also need some helper functions to perform the sampling from the mixture of logistics as well as the looping to generate the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1ZAxIQIPX-L"
      },
      "source": [
        "def sample_mol(y_hat, num_classes=256):\n",
        "    \"\"\"Sample from mixture of logistics.\n",
        "\n",
        "    y_hat: NxC where C is 3*number of logistics\n",
        "    \"\"\"\n",
        "    assert len(y_hat.shape) == 2\n",
        "\n",
        "    N = y_hat.size(0)\n",
        "    nr_mix = y_hat.size(1) // 3\n",
        "\n",
        "    probs = torch.softmax(y_hat[:, :nr_mix], dim=-1)\n",
        "    means = y_hat[:, nr_mix:2 * nr_mix]\n",
        "    scales = torch.nn.functional.elu(y_hat[:, 2*nr_mix:3*nr_mix]) + 1.0001\n",
        "\n",
        "    indices = torch.multinomial(probs, 1).squeeze()\n",
        "    batch_indices = torch.arange(N, device=probs.device)\n",
        "    mu = means[batch_indices, indices]\n",
        "    s = scales[batch_indices, indices]\n",
        "    u = torch.rand(N, device=probs.device)\n",
        "    preds = mu + s*(torch.log(u) - torch.log(1-u))\n",
        "\n",
        "    return torch.min(\n",
        "        torch.max(\n",
        "            torch.round((preds+1)/2*(num_classes-1)),\n",
        "            preds.new_zeros(1),\n",
        "        ),\n",
        "        preds.new_ones(1)*(num_classes-1)\n",
        "    ).long().view(N, 1)\n",
        "\n",
        "\n",
        "def predict_with_recurrent(model, images, n):\n",
        "    memory = None\n",
        "    y_hat = []\n",
        "    x_hat = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(n):\n",
        "            x_hat.append(images[:, i:i+1])\n",
        "            yi, memory = model(x_hat[-1], i=i, memory=memory)\n",
        "            y_hat.append(yi)\n",
        "\n",
        "        for i in range(n, images.shape[1]):\n",
        "            x_hat.append(sample_mol(y_hat[-1], 256))\n",
        "            yi, memory = model(x_hat[-1], i=i, memory=memory)\n",
        "            y_hat.append(yi)\n",
        "\n",
        "        x_hat.append(sample_mol(y_hat[-1], 256))\n",
        "        x_hat = torch.stack(x_hat, dim=1)\n",
        "\n",
        "    return x_hat\n",
        "\n",
        "\n",
        "def predict(model, images, n):\n",
        "    N, L = images.shape\n",
        "    x_hat = images.new_zeros(N, L+1, dtype=torch.long)\n",
        "    x_hat[:, :n] = images[:, :n]\n",
        "    with torch.no_grad():\n",
        "        for i in range(n, L):\n",
        "            y_hat = model(x_hat[:, :i])\n",
        "            x_hat[:, i:i+1] = sample_mol(y_hat[:,-1,:], 256)\n",
        "        x_hat[:, -1:] = sample_mol(y_hat[:,-1,:], 256)\n",
        "    return x_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eTKeBebGY8e"
      },
      "source": [
        "## Loading pretrained models\n",
        "\n",
        "After defining our modules we can now download our pretrained models from Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5to7CKVIfea"
      },
      "source": [
        "import io\n",
        "import requests\n",
        "\n",
        "LINEAR_MODEL = \"https://drive.google.com/uc?export=download&id=17fc94TzytTdAwNMVCE7qOg75-CWLGi_p\"\n",
        "SOFTMAX_MODEL = \"https://drive.google.com/uc?export=download&id=1L47Ode6GxCMQbVMK33_ANjCu2iA4rf8l\"\n",
        "\n",
        "linear_weights = torch.load(io.BytesIO(requests.get(LINEAR_MODEL).content))\n",
        "softmax_weights = torch.load(io.BytesIO(requests.get(SOFTMAX_MODEL).content))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS6rDUbtOkRK"
      },
      "source": [
        "Now, let's create the model and generate some images. Note that we are creating a recurrent model for softmax. This means that we save all keys and values to avoid computing them again which is not something easily done for every transformer implementation (for instance reformer).\n",
        "\n",
        "On the other hand, for linear attention the state has fixed size and it is natural to implement it as a recurrent model (see section 3.4 in [our paper](https://arxiv.org/pdf/2006.16236.pdf))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq-8xxuZO03b"
      },
      "source": [
        "linear = RecurrentGenerator(256, 783, 10, \"linear\", 8, 8)\n",
        "linear.load_state_dict(linear_weights)\n",
        "linear.eval()\n",
        "full = RecurrentGenerator(256, 783, 10, \"full\", 8, 8)\n",
        "full.load_state_dict(softmax_weights)\n",
        "full.eval()\n",
        "\n",
        "images_linear = predict_with_recurrent(linear, torch.zeros(1, 783, dtype=torch.int64), 1)\n",
        "images_full = predict_with_recurrent(full, torch.zeros(1, 783, dtype=torch.int64), 1)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].set_title(\"Linear\")\n",
        "ax[0].imshow(images_linear[0].cpu().numpy().reshape(28, 28))\n",
        "ax[1].set_title(\"Softmax\")\n",
        "ax[1].imshow(images_full[0].cpu().numpy().reshape(28, 28))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSb631C7V86M"
      },
      "source": [
        "## Time measurements\n",
        "\n",
        "After validating that our models work and generate proper images, let's do some time measurements. Let's measure how much time takes each method to generate 100 images (linear takes about 30 seconds and softmax about 200 so please be patient).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OST94DdOWSQQ"
      },
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "images_linear = predict_with_recurrent(linear, torch.zeros(100, 783, dtype=torch.int64), 1)\n",
        "end = time.time()\n",
        "print(\"Linear took\", round(end-start, 2), \"s\")\n",
        "\n",
        "start = time.time()\n",
        "images_full = predict_with_recurrent(full, torch.zeros(100, 783, dtype=torch.int64), 1)\n",
        "end = time.time()\n",
        "print(\"Stateful-softmax took\", round(end-start, 2), \"s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etE-86blZbU7"
      },
      "source": [
        "Note that all those computations have been using the colab CPU. Let's run the same experiment with the GPU instead.\n",
        "\n",
        "Our linear model uses constant memory throughout the prediction. This means that as we increase the sequence length or the batch size the speedup will only increase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvvYJzLfZv9l"
      },
      "source": [
        "# Transfer the models to the GPU\n",
        "linear.cuda()\n",
        "full.cuda()\n",
        "\n",
        "# Do some warmup before taking the measurments\n",
        "predict_with_recurrent(linear, torch.zeros(1, 783, dtype=torch.int64, device=\"cuda\"), 1)\n",
        "predict_with_recurrent(full, torch.zeros(1, 783, dtype=torch.int64, device=\"cuda\"), 1)\n",
        "\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "start.record()\n",
        "images_linear = predict_with_recurrent(linear, torch.zeros(500, 783, dtype=torch.int64, device=\"cuda\"), 1)\n",
        "end.record()\n",
        "torch.cuda.synchronize()\n",
        "del images_linear\n",
        "print(\"Linear took\", round(start.elapsed_time(end)/1000, 2), \"s\")\n",
        "\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "start.record()\n",
        "images_full = predict_with_recurrent(full, torch.zeros(500, 783, dtype=torch.int64, device=\"cuda\"), 1)\n",
        "end.record()\n",
        "torch.cuda.synchronize()\n",
        "del images_full\n",
        "print(\"Stateful-softmax took\", round(start.elapsed_time(end)/1000, 2), \"s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPs9vWGUc8IM"
      },
      "source": [
        "# Micro-benchmark\n",
        "\n",
        "Let's finish up this demonstration with a micro-benchmark for the training time of the transformers.\n",
        "\n",
        "Firstly, we create some helper functions to measure the time for the benchmark. `bench()` measures the time required to perform a forward/backward pass for a variety of sequence lengths and batch sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mD1O9AihEmo"
      },
      "source": [
        "def bench_one(model, x):\n",
        "  # warmup the caches\n",
        "  y = model(x)\n",
        "  y.sum().backward()\n",
        "\n",
        "  start = torch.cuda.Event(enable_timing=True)\n",
        "  end = torch.cuda.Event(enable_timing=True)\n",
        "  start.record()\n",
        "  y = model(x)\n",
        "  y.sum().backward()\n",
        "  end.record()\n",
        "  torch.cuda.synchronize()\n",
        "\n",
        "  return start.elapsed_time(end)/1000\n",
        "\n",
        "def bench(model, batches, sequence_lengths):\n",
        "  time = []\n",
        "  for b, s in zip(batches, sequence_lengths):\n",
        "    x = torch.rand(b, s, 768).cuda()\n",
        "    time.append(bench_one(model, x)/b)\n",
        "  return time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS-wM6mCiDms"
      },
      "source": [
        "Now, we can create the transformers to be tested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sUaLZtHiKqz"
      },
      "source": [
        "builder = TransformerEncoderBuilder.from_kwargs(\n",
        "    n_layers=1,\n",
        "    n_heads=12,\n",
        "    query_dimensions=64,\n",
        "    value_dimensions=64,\n",
        "    feed_forward_dimensions=768\n",
        ")\n",
        "\n",
        "builder.attention_type = \"full\"\n",
        "full = builder.get().cuda()\n",
        "builder.attention_type = \"linear\"\n",
        "linear = builder.get().cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4uuEtSG0kJq"
      },
      "source": [
        "Finally, we just select batch sizes such that we do not get GPU out of memory errors and we measure the time for a forward/backward pass for increasing sequence lengths.\n",
        "\n",
        "From the plot, we observe that linear scales indeed linearly with respect to the sequence length while the full attention scales quadratically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY1S9G0RjNHE"
      },
      "source": [
        "sequence_lengths = [2**i for i in range(7, 13)]\n",
        "linear_batches = [1000, 500, 250, 125, 62, 31]\n",
        "softmax_batches = [1000, 300, 100, 25, 5, 1]\n",
        "\n",
        "linear_time = bench(linear, linear_batches, sequence_lengths)\n",
        "softmax_time = bench(full, softmax_batches, sequence_lengths)\n",
        "\n",
        "plt.plot(sequence_lengths, softmax_time, label=\"softmax\")\n",
        "plt.plot(sequence_lengths, linear_time, label=\"linear\")\n",
        "plt.xlabel(\"Sequence Length\")\n",
        "plt.ylabel(\"Seconds\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}